project: bria
batch_size: 16
seed: 17
scale_schedule_ratio: 1.0
name: test
eval_first: false
algorithms:
  low_precision_groupnorm:
    attribute: unet
    precision: amp_fp16
  low_precision_layernorm:
    attribute: unet
    precision: amp_fp16
model:
  _target_: diffusion.models.models.stable_diffusion_2
  pretrained: false
  precomputed_latents: false
  encode_latents_in_fp16: true
  fsdp: true
  val_metrics:
    - _target_: torchmetrics.MeanSquaredError
  val_guidance_scales: []
  loss_bins: []
dataset:
  train_batch_size: ${batch_size}
  eval_batch_size: ${batch_size}
  train_dataset:
    _target_: diffusion.datasets.bria.bria_loader.dataset
    # remote:
    # Path to object store bucket(s)
    local:
      /data
      # Path to corresponding local dataset(s)
    seed: ${seed}
    batch_size: ${batch_size}
    tokenizer_name_or_path: stabilityai/stable-diffusion-2-base
    tokenizer_revision: main
    resolution: 256
    center_crop: true
    random_flip: true
    drop_last: true
    shuffle: true
    num_workers: 1
    pin_memory: true
  eval_dataset:
    _target_: diffusion.datasets.bria.bria_loader.dataset
    local: /data
    batch_size: ${batch_size}
    tokenizer_name_or_path: stabilityai/stable-diffusion-2-base
    tokenizer_revision: main
    resolution: 256
    center_crop: true
    random_flip: true
    drop_last: true
    shuffle: true
    num_workers: 1
    pin_memory: true
optimizer:
  _target_: torch.optim.AdamW
  lr: 1.0e-4
  weight_decay: 0.01
scheduler:
  _target_: composer.optim.MultiStepWithWarmupScheduler
  t_warmup: 10000ba
  milestones:
    - 200ep
logger:
  wandb:
    _target_: composer.loggers.wandb_logger.WandBLogger
    name: ${name}
    project: ${project}
    group: ${name}
callbacks:
  speed_monitor:
    _target_: composer.callbacks.speed_monitor.SpeedMonitor
    window_size: 10
  lr_monitor:
    _target_: composer.callbacks.lr_monitor.LRMonitor
  memory_monitor:
    _target_: composer.callbacks.memory_monitor.MemoryMonitor
  runtime_estimator:
    _target_: composer.callbacks.runtime_estimator.RuntimeEstimator
  optimizer_monitor:
    _target_: composer.callbacks.OptimizerMonitor
trainer:
  _target_: composer.Trainer
  device: gpu
  eval_subset_num_batches: 5
  max_duration: 550000ba
  device_train_microbatch_size: 16
  run_name: ${name}
  seed: ${seed}
  scale_schedule_ratio: ${scale_schedule_ratio}
  save_folder: /tmp
  save_interval: 10000ba
  save_overwrite: true
  autoresume: false
  fsdp_config:
    sharding_strategy: "SHARD_GRAD_OP"
